{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0bb7a60ed8644274dfb8d0f077341046c00c278fb1a08c2f4e6b2c86a22654d65",
   "display_name": "Python 3.8.6  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bb7a60ed8644274dfb8d0f077341046c00c278fb1a08c2f4e6b2c86a22654d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, decomposition, cluster\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Load Dataset\n",
    "Load the Million Song Dataset from 'YearPredictionsMSP.txt' (expected to be in the data folder)  \n",
    "Dataset Source: https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD  \n",
    "\n",
    "The following information is from the UCI webpage for this dataset (linked above):\n",
    "\n",
    "## Data Set Information:\n",
    "You should respect the following train / test split:  \n",
    "train: first 463,715 examples  \n",
    "test: last 51,630 examples  \n",
    "It avoids the 'producer effect' by making sure no song  \n",
    "from a given artist ends up in both the train and test set.  \n",
    "\n",
    "## Attribute Information:  \n",
    "90 attributes, 12 = timbre average, 78 = timbre covariance  \n",
    "The first value is the year (target), ranging from 1922 to 2011.  \n",
    "Features extracted from the 'timbre' features from The Echo Nest API.  \n",
    "We take the average and covariance over all 'segments', each segment  \n",
    "being described by a 12-dimensional timbre vector.  \n",
    "\n",
    "## Relevant Papers:\n",
    "see the website: http://millionsongdataset.com/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the text file\n",
    "raw_df = pd.read_csv(\"./data/YearPredictionMSD.txt\", header=None)\n",
    "years = raw_df[0].to_numpy(dtype=int)\n",
    "\n",
    "# Drop the year column and assign the attribute data to a numpy ndarray\n",
    "raw_df.drop(0, axis=1, inplace=True)\n",
    "attribute_data = raw_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the raw data into training and test sets\n",
    "DataContainer = namedtuple(\"DataContainer\", [\"years\", \"data\"])\n",
    "TRAIN_IDX = np.arange(0, 463715, dtype=int)\n",
    "TEST_IDX = np.arange(463715, 463715 + 51630, dtype=int)\n",
    "\n",
    "# Split data into training and test set, and also separate the year labels\n",
    "# from the attributes\n",
    "train = DataContainer(years[TRAIN_IDX], attribute_data[TRAIN_IDX])\n",
    "test = DataContainer(years[TEST_IDX], attribute_data[TEST_IDX])\n",
    "\n",
    "# Print the shape of the training and test data to ensure it\n",
    "# was loaded correctly. Expect 463715/51630 rows, and 90 columns\n",
    "print(f\"Train Data Shape: {train.data.shape}\")\n",
    "print(f\"Test Data Shape:  {test.data.shape}\")"
   ]
  },
  {
   "source": [
    "## Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training and test data\n",
    "preprocessing.normalize(train.data, norm='l2', axis=1, copy=False)\n",
    "preprocessing.normalize(test.data, norm='l2', axis=1, copy=False)"
   ]
  },
  {
   "source": [
    "## Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=25)\n",
    "pca.fit(train.data)\n",
    "train_pca = DataContainer(train.years, pca.transform(train.data))\n",
    "print(f\"Training Data PCA Total Explained Variance Ratio = {pca.explained_variance_ratio_.sum()}\")\n",
    "\n",
    "pca = decomposition.PCA(n_components=25)\n",
    "pca.fit(train.data)\n",
    "test_pca = DataContainer(test.years, pca.transform(test.data))\n",
    "print(f\"Test Data PCA Total Explained Variance Ratio = {pca.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "source": [
    "# Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=8)\n",
    "kmeans.fit(train.data)"
   ]
  },
  {
   "source": [
    "# Results Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in set(kmeans.labels_):\n",
    "    x = train.years[kmeans.labels_ == i]\n",
    "    y = np.random.randn(x.size)\n",
    "    plt.scatter(x, y)"
   ]
  }
 ]
}